# hadoop安装
## 前言
此文档主要是基于 `Hadoop-2.7.7` 安装的指示文档

## 硬件需求
参数\配置|最低|一般|推荐
:-:|:-:|:-:|:-:
CPU|2 * (6 core 12 thread)|2 * (8 core 16 thread)|2 * (12 core 24 thread)
内存|64G|128G|256G
存储|2 system disk + 12T|2 system disk + 24T|2 system disk + 48T
网络|	两端口千兆网卡|	两端口千兆网卡|	四端口千兆网卡

注意：
1. 关于存储，建议减少单块磁盘的大小，增加磁盘的数量、单块在1-2T最合适。
2. 服务器网卡要跟服务器存储总的吞吐量想匹配。
3. 实际最低配置要求还需要跟据所需要用到的模块，以及配置的资源，除去系统最低的占用资源，进行计算。

## 操作系统调优
### 修改ulimit
&emsp;操作系统默认的ulimit比较小，容易引起"Too many open files"错误。
### 修改90-nproc.conf
> 修改/etc/security/limits.d/90-nproc.conf，主要是把操作系统默认的对其它用户的1024的限制去掉。
```shell
[root@MASTER ~]# vi /etc/security/limits.d/90-nproc.conf
# Default limit for number of user's processes to prevent
# accidental fork bombs.
# See rhbz #432903 for reasoning.

#*          soft    nproc     1024
root       soft    nproc     unlimited
```

### 修改limits.conf
> 修改/etc/security/limits.conf主要是调大hadoop用户的最大打开文件数限制。
```shell
[root@ONE ~]# vi /etc/security/limits.conf

#<domain>      <type>  <item>         <value>
#

#*               soft    core            0
#*               hard    rss             10000
#@student        hard    nproc           20
#@faculty        soft    nproc           20
#@faculty        hard    nproc           50
#ftp             hard    nproc           0
#@student        -       maxlogins       4

# End of file

youruser soft nofile 1048576
youruser hard nofile 1048576
youruser soft nproc 1048576
youruser hard nproc 1048576
```

### 文件系统添加noatime
> 当文件被创建，修改和访问时，Linux系统会记录这些时间信息。当系统的读文件操作频繁时，记录文件最近一次被读取的时间信息，将是一笔不少的开销。所以，为了提高系统的性能，我们可以在读取文件时不修改文件的atime属性。可以通过在加载文件系统时使用noatime选项来做到这一点。当以noatime选项加载（mount）文件系统时，对文件的读取不会更新文件属性中的atime信息。注意wtime信息仍然有效，任何时候文件被写，该信息仍被更新。

用下面的命令可以验证是否已经加上了：
```shell
#验证
[root@ONE ~]# mount -l
/dev/md0 on / type ext4 (rw)
proc on /proc type proc (rw)
sysfs on /sys type sysfs (rw)
devpts on /dev/pts type devpts (rw,gid=5,mode=620)
tmpfs on /dev/shm type tmpfs (rw)
/dev/sda1 on /opt/hadoop/hadoop-data/datanode/data1 type ext4 (rw,noatime)
/dev/sdb1 on /opt/hadoop/hadoop-data/datanode/data2 type ext4 (rw,noatime)
...
```
如果没加上，可按如下步骤动态添加noatime属性：
```shell
#编辑/etc/fstab，添加noatime属性
[root@ONE ~]# vi /etc/fstab
#
# /etc/fstab
# Created by anaconda on Tue Apr 21 16:40:06 2015
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
UUID=d8780a72-c3dd-4bbb-96b3-89e378891905 /                       ext4    defaults        1 1
#UUID=6b22ab31-1f91-4ef6-b0f4-749cfabe7998 swap                    swap    defaults        0 0
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
UUID=0b7c8399-36a0-411f-a7af-92ff5e867537 /opt/hadoop/hadoop-data/datanode/data1               ext4  defaults,noatime  0    0
UUID=258ee91a-f23a-4986-a379-eb4bbf32d066 /opt/hadoop/hadoop-data/datanode/data2               ext4  defaults,noatime  0    0
UUID=6111ef8d-abe9-4a0b-888f-fc500ef5f5b2 /opt/hadoop/hadoop-data/datanode/data3               ext4  defaults,noatime  0    0
...

#动态生效
[root@ONE ~]# df -k | grep "/opt/hadoop/hadoop-data/datanode/data"  | awk '{ print "mount -o remount "$1 }' | sh

#验证
mount -l
```

### 关闭swap
>在Linux中，如果一个进程的内存空间不足，那么，它会将内存中的部分数据暂时写到磁盘上，当需要时，再将磁盘上的数据动态置换到内存中，通常而言，这种行为会大大降低进程的执行效率。

因此流媒体服务器不需要使用swap，需要检查是否关闭，如果没有关闭，按如下方法关闭。
```shell
#检查是否启用swap
[root@NC-STRM1 ~]# swapon -s
Filename                                Type            Size    Used    Priority
/dev/sda3                               partition       24575996        1033708 -1

#关闭swap
[root@NC-STRM1 ~]# swapoff /dev/sda3

#关闭成功后
[root@NC-STRM1 ~]# swapon -s
Filename                                Type            Size    Used    Priority
[root@NC-STRM1 ~]#

#修改/etc/fstab文件，关闭启动加载。（参考）
[root@NC-STRM1 ~]# cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Mon Aug 24 16:35:23 2015
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
UUID=7de7e855-5283-4d5e-bfc1-c06224f8b22c /                       ext4    defaults        1 1
UUID=67657fdf-64b5-4694-8618-10888e8a12d8 /opt                    ext4    defaults        1 2
#UUID=213a5ee8-574f-4a64-8d12-cb786820021f swap                    swap    defaults        0 0
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
..
```
###  (可选)更换操作系统du命令
> HDFS在运行过程中，datanode经常会执行"du -sk"命令获取每个datanode数据存储目录的大小。随着datanode上的数据的增加，频繁的"du -sk"命令非常消耗CPU资源，会导致磁盘IO和CPU飙升，影响业务运行。
这里用了一个shell脚本通过"df -k"来实现"du -sk"的功能，这样虽然可能不准，但是datanode的数据存储目录一般都是单独占用一个硬盘，因此这么做的误差应该是极小的。这个脚本位置在/opt/hadoop/hadoop-2.7.7/sbin/du.sh。用这个脚本替换操作系统du命令即可。
* 替换过程
```SHELL
#替换
[root@MASTER ~]# mv /usr/bin/du /usr/bin/du_20160616_bk
[root@MASTER ~]# cp /opt/hadoop/hadoop-2.7.7/sbin/du.sh /usr/bin/du

#验证
[root@MASTER ~]# du -sk /opt
60806372        /opt
[root@MASTER ~]# df -k /opt
Filesystem     1K-blocks     Used Available Use% Mounted on
/dev/md0       302248248 60806368 226081888  22% /
```
* 脚本内容
```SHELL
#脚本内容
[root@SIT-STRM4 ~]# cat /usr/bin/du
#!/bin/sh

mydf=$(df -Pk $2 | grep -vE '^Filesystem|tmpfs|cdrom' | awk '{ print $3 }')
echo -e "$mydf\t$2"
```

## jdk 安装
<p style="display:inline">详情查看</p> <p style="color: #66ccff; display:inline">Java 安装</p>

## Zookeeper 安装
<p style="display:inline">详情查看</p> <p style="color: #66ccff; display:inline">Zookeeper 安装</p>

## 集群安装
### ssh 免密
<p style="display:inline">详情查看</p> <p style="color: #66ccff; display:inline">Hbase安装文档.ssh免密登陆</p>

### 设置HADOOP_HOME
<p style="display:inline">详情查看</p> <p style="color: #66ccff; display:inline">Java 安装</p> 中的增加环境变量

```shell
JAVA_HOME=/opt/hadoop/jdk1.8.0_172
export JAVA_HOME

ZOOKEEPER_HOME=/opt/hadoop/zookeeper-3.5.7
export ZOOKEEPER_HOME

HADOOP_HOME=/opt/hadoop/hadoop-2.7.7
export HADOOP_HOME

PATH=$JAVA_HOME/bin:$PATH:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export PATH
```

### 安装列表
<table>
  <thead>
    <tr>
      <th>应用</th>
      <th>模块/服务器</th>
      <th>server01</th>
      <th>server02</th>
      <th>server03</th>
      <th>server04</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Zookeeper</td>
      <td>Zookeeper</td>
      <td>x</td>
      <td>x</td>
      <td>x</td>
      <td></td>
    </tr>
    <tr>
     <td rowspan="4">HDFS</td>
      <td>NameNode</td>
      <td>x</td>
      <td>x</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>JournalNode</td>
      <td>x</td>
     <td>x</td>
     <td>x</td>
     <td></td>
    </tr>
    <tr>
      <td>ZKFC</td>
      <td>x</td>
      <td>x</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>DataNode</td>
      <td>x</td>
      <td>x</td>
      <td>x</td>
      <td>x</td>
    </tr>
    <tr>
      <td rowspan="3">YARN</td>
      <td>ResourceManager</td>
      <td>x</td>
      <td>x</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>NodeManager</td>
      <td>x</td>
      <td>x</td>
      <td>x</td>
      <td>x</td>
    </tr>
    <tr>
      <td>JobHistoryServer</td>
      <td>x</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>

### 模块端口
&emsp;见配置文件中，具体配置项。

### Datanode数据硬盘初始化
Datanode数据硬盘初始化分三步：
* 硬盘分区
* 硬盘格式化
* 挂载硬盘分区
<p style="display:inline">详情查看</p> <p style="color: #66ccff; display:inline">datanode数据硬盘初始化</p>

### 主要配置
#### Hadoop JVM GC配置(可选)
> Hadoop的HDFS相关的模块都是java开发，因此需要对JVM的GC参数进行调优。HDFS的JVM GC调优在安装包里已经完成了，这里需要做的就是分别为NameNode、SecondaryNameNode和DataNode分配运行内存。Jvm内存的分配要根据服务器内存大小和Hdfs block数量决定。

>一般不需要修改，用默认调优即可。除非block数量超过1000万。
```shell
# 查看block数量
hadoop fsck -blocks /
```
如果需，修改/opt/hadoop/hadoop-2.7.7/sbin/hadoop-daemon.sh如下set jvm gc部分，分别为NameNode和DataNode设置运行内存。
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/sbin/hadoop-daemon.sh

#set jvm gc
if [ "$command" == "datanode" ]; then
    export HADOOP_HEAPSIZE=4096
    GC_OPTS="-Xms4096M $GC_OPTS"
elif [ "$command" == "namenode" ]; then
    export HADOOP_HEAPSIZE=6144
    GC_OPTS="-Xms6144M $GC_OPTS"
elif [ "$command" == "secondarynamenode" ]; then
    export HADOOP_HEAPSIZE=4096
    GC_OPTS="-Xms3072M $GC_OPTS"
else
    export HADOOP_HEAPSIZE=1024
    GC_OPTS="-Xms256M $GC_OPTS"
fi
```

#### 配置slaves
> 配置slaves就是告诉namenode那些服务器要加入hdfs作为DataNode，把DataNode服务器主机名加入到/opt/hadoop/hadoop-2.7.7/etc/hadoop/slaves文件。
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/slaves
ONE
TWO
THREE
FOUR
```

####  配置两台 NameNode
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/hdfs-site.xml
...
    <!-- variable -->
    <property>
        <name>dfs.namenode.hostname.nn1</name>
        <value>ONE</value>
    </property>
    <property>
        <name>dfs.namenode.hostname.nn2</name>
        <value>TWO</value>
    </property>
...
```

#### 配置NameNode zookeeper集群地址
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/hdfs-site.xml
...
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>ONE:2181,TWO:2181,THREE:2181</value>
    </property>
...
```

#### 配置journalnode集群地址
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/hdfs-site.xml
...
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://ONE:8045;TWO:8045;THREE:8045/bicluster</value>
    </property>
...
```

#### 配置本地DataNode
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/hdfs-site.xml
...
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/opt/hadoop/hadoop-data/datanode/data1,/opt/hadoop/hadoop-data/datanode/data2,/opt/hadoop/hadoop-data/datanode/data3,/opt/hadoop/hadoop-data/datanode/data4</value>
    </property>
...
```

#### 配置yarn-include
> 配置yarn-include就是告诉resourcemanager那些服务器要加入yarn集群作为nodemanager，把nodemanager服务器主机名加入到/opt/hadoop/hadoop-2.7.7/etc/hadoop/ yarn-include文件。
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/yarn-include
ONE
TWO
THREE
```

#### 配置两台 ResourceManager
```shell
...
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>ONE</value>
    </property>
    ...
    <property>
        <name>yarn.resourcemanager.ha.id</name>
        <value>rm1</value>
    </property>
```
* 第二台要记得修改名称为第二台的主机名
```shell
...
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>TWO</value>
    </property>
...
```

#### 配置ResourceManager zookeeper集群地址
```SHELL
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/yarn-site.xml
...
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>ONE:2181,TWO:2181,THREE:2181</value>
    </property>
...
```

#### 配置YARN JobHistoryServer
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/mapred-site.xml
...
    <property>
            <name>mapreduce.jobhistory.address</name>
            <value>ONE:10020</value>
    </property>
    <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>ONE:19888</value>
    </property>
...
```

#### 配置NodeManager
> 配置ResourceManager zookeeper集群地址主要是修改配置文件/opt/hadoop/hadoop-2.7.7/etc/hadoop/yarn-site.xml的如下配置项。根据服务器的实际硬件配置和负载进行配置。
>* yarn.nodemanager.resource.cpu-vcores = total-cpu-vcores *0.8
>* 可以通过cat /proc/cpuinfo| grep "processor"| wc -l查看total-cpu-vcores，一般每个节点可使用cpu-vcores大概为total-cpu-vcores的80%。例如：总逻辑CPU数40可以为配置为32, 总数48可以配置为40。
>* yarn.scheduler.minimum-allocation-mb = 4096/2 = 2048
单个任务可申请最少内存，可以根据实际内存大小配置。但不大于总内存/总cup数
>* yarn.scheduler.maximum-allocation-mb = yarn.nodemanager.resource.cpu-vcores * 4096
>* 单个任务可申请最大内存=分配cpu数*最少内存
>* yarn.nodemanager.resource.memory-mb = yarn.scheduler.maximum-allocation-mb
>* yarn.nodemanager.local-dirs
>* yarn.nodemanager.log-dirs
```shell
[hadoop@ONE ~]$ vi /opt/hadoop/hadoop-2.7.7/etc/hadoop/yarn-site.xml
...
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>16</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>65536</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>65536</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2048</value>
    </property>
    ...
    <!-- NodeManager log configuration -->
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/opt/hadoop/hadoop-data/datanode/data1/yarn/local,/opt/hadoop/hadoop-data/datanode/data2/yarn/local,/opt/hadoop/hadoop-data/datanode/data3/yarn/local,/opt/hadoop/hadoop-data/datanode/data4/yarn/local</value>
    </property>
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/opt/hadoop/hadoop-data/datanode/data1/yarn/logs,/opt/hadoop/hadoop-data/datanode/data2/yarn/logs,/opt/hadoop/hadoop-data/datanode/data3/yarn/logs,/opt/hadoop/hadoop-data/datanode/data4/yarn/logs</value>
    </property>
...
```

#### 配置MapReduce
```shell
[hadoop@ONE hadoop]$ vi mapred-site.xml

 <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>ONE:10020</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>ONE:19888</value>
  </property>
```
## HDFS初始化
> 在安装配置完成之后，第一次运行HDFS之前，要对HDFS进行初始化。基本步骤如下：
1. 启动JournalNode
2. NameNode nn1执行初始化
3. NameNode nn1启动namenode
4. NameNode nn2 执行bootstrapStandby

### 启动JournalNode
```shell
su - hadoop
cd hadoop-2.7.7/sbin/
./hadoop-daemon.sh start journalnode
```
* 检查
```shell
# 进程
ps -ef | grep journalnode | grep -v grep

# 端口
ss -anp sport = :8480 or sport = :8485

#日志
tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-journalnode-$(hostname).log
```

### NamNode nn1执行初始化
* 初始化命令
```shell
su - hadoop
cd hadoop-2.7.7/bin/
./hdfs namenode -format
```
* 初始化日志
注意日志中这行记录代表初始化成功: `common.Storage: Storage directory /opt/hadoop/hadoop-data/namenode/data has been successfully formatted`.
```shell
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/bin/
[hadoop@ONE bin]$ ./hdfs namenode -format
20/03/21 18:57:40 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ONE/0.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2020-03-18T09:56Z
STARTUP_MSG:   java = 1.8.0_172
************************************************************/
20/03/21 18:57:40 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
20/03/21 18:57:40 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-08df6c0d-1db9-40f5-9350-797a3354ff76
20/03/21 18:57:40 INFO namenode.FSNamesystem: No KeyProvider found.
20/03/21 18:57:40 INFO namenode.FSNamesystem: fsLock is fair: true
20/03/21 18:57:40 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
20/03/21 18:57:40 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
20/03/21 18:57:40 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
20/03/21 18:57:40 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
20/03/21 18:57:40 INFO blockmanagement.BlockManager: The block deletion will start around 2020 三月 21 18:57:40
20/03/21 18:57:40 INFO util.GSet: Computing capacity for map BlocksMap
20/03/21 18:57:40 INFO util.GSet: VM type       = 64-bit
20/03/21 18:57:40 INFO util.GSet: 2.0% max memory 958.5 MB = 19.2 MB
20/03/21 18:57:40 INFO util.GSet: capacity      = 2^21 = 2097152 entries
20/03/21 18:57:40 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
20/03/21 18:57:40 INFO blockmanagement.BlockManager: defaultReplication         = 3
20/03/21 18:57:40 INFO blockmanagement.BlockManager: maxReplication             = 512
20/03/21 18:57:40 INFO blockmanagement.BlockManager: minReplication             = 1
20/03/21 18:57:40 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
20/03/21 18:57:40 INFO blockmanagement.BlockManager: replicationRecheckInterval = 5000
20/03/21 18:57:40 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
20/03/21 18:57:40 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
20/03/21 18:57:40 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
20/03/21 18:57:40 INFO namenode.FSNamesystem: supergroup          = supergroup
20/03/21 18:57:40 INFO namenode.FSNamesystem: isPermissionEnabled = false
20/03/21 18:57:40 INFO namenode.FSNamesystem: Determined nameservice ID: bicluster
20/03/21 18:57:40 INFO namenode.FSNamesystem: HA Enabled: true
20/03/21 18:57:40 INFO namenode.FSNamesystem: Append Enabled: true
20/03/21 18:57:41 INFO util.GSet: Computing capacity for map INodeMap
20/03/21 18:57:41 INFO util.GSet: VM type       = 64-bit
20/03/21 18:57:41 INFO util.GSet: 1.0% max memory 958.5 MB = 9.6 MB
20/03/21 18:57:41 INFO util.GSet: capacity      = 2^20 = 1048576 entries
20/03/21 18:57:41 INFO namenode.FSDirectory: ACLs enabled? false
20/03/21 18:57:41 INFO namenode.FSDirectory: XAttrs enabled? false
20/03/21 18:57:41 INFO namenode.FSDirectory: Maximum size of an xattr: 16384
20/03/21 18:57:41 INFO namenode.NameNode: Caching file names occuring more than 10 times
20/03/21 18:57:41 INFO util.GSet: Computing capacity for map cachedBlocks
20/03/21 18:57:41 INFO util.GSet: VM type       = 64-bit
20/03/21 18:57:41 INFO util.GSet: 0.25% max memory 958.5 MB = 2.4 MB
20/03/21 18:57:41 INFO util.GSet: capacity      = 2^18 = 262144 entries
20/03/21 18:57:41 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.22200000286102295
20/03/21 18:57:41 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
20/03/21 18:57:41 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
20/03/21 18:57:41 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
20/03/21 18:57:41 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
20/03/21 18:57:41 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
20/03/21 18:57:41 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
20/03/21 18:57:41 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
20/03/21 18:57:41 INFO util.GSet: Computing capacity for map NameNodeRetryCache
20/03/21 18:57:41 INFO util.GSet: VM type       = 64-bit
20/03/21 18:57:41 INFO util.GSet: 0.029999999329447746% max memory 958.5 MB = 294.5 KB
20/03/21 18:57:41 INFO util.GSet: capacity      = 2^15 = 32768 entries
20/03/21 18:57:42 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1222102639-0.0.0.1-1584788262066
20/03/21 18:57:42 INFO common.Storage: Storage directory /opt/hadoop/hadoop-data/namenode/data has been successfully formatted.
20/03/21 18:57:42 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hadoop/hadoop-data/namenode/data/current/fsimage.ckpt_0000000000000000000 using no compression
20/03/21 18:57:42 INFO namenode.FSImageFormatProtobuf: Image file /opt/hadoop/hadoop-data/namenode/data/current/fsimage.ckpt_0000000000000000000 of size 322 bytes saved in 0 seconds.
20/03/21 18:57:42 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
20/03/21 18:57:42 INFO util.ExitUtil: Exiting with status 0
20/03/21 18:57:42 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ONE/0.0.0.1
************************************************************/
```

### NameNode nn1启动namenode
* 启动namenode命令
```shell
su – hadoop
cd hadoop-2.7.7/sbin/
./hadoop-daemon.sh start namenode
```

* 启动namenode日志
```shell
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/sbin/
[hadoop@ONE sbin]$ ./hadoop-daemon.sh start namenode
HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
starting namenode, logging to /opt/hadoop/hadoop-2.7.7/logs/hadoop-hadoop-namenode-ONE.out
```

* 检查
```shell
# 进程
ps -ef | grep namenode | grep -v grep

# 端口
ss -anp sport = :8020 or sport = :8021 or sport = :50070

#日志
tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-namenode-$(hostname).log
```

### NameNode nn2 执行bootstrapStandby
> nn2服务器不需要初始化，但是要执行命令 `hdfs namenode -bootstrapStandby`
* bootstrapStandby命令
```shell
#在nn2上
su - hadoop
cd hadoop-2.7.7/bin/
./hdfs namenode -bootstrapStandby
```

* bootstrapStandby日志
```shell
[root@TWO ~]# su - hadoop
[hadoop@TWO ~]$ cd hadoop-2.7.7/bin
[hadoop@TWO bin]$ ./hdfs namenode -bootstrapStandby
20/03/21 19:08:07 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = TWO/0.0.0.2
STARTUP_MSG:   args = [-bootstrapStandby]
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2020-03-18T09:56Z
STARTUP_MSG:   java = 1.8.0_172
************************************************************/
20/03/21 19:08:07 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
20/03/21 19:08:07 INFO namenode.NameNode: createNameNode [-bootstrapStandby]
=====================================================
About to bootstrap Standby ID nn2 from:
           Nameservice ID: bicluster
        Other Namenode ID: nn1
  Other NN's HTTP address: http://ONE:50070
  Other NN's IPC  address: ONE/0.0.0.1:8021
             Namespace ID: 10029943
            Block pool ID: BP-1222102639-0.0.0.1-1584788262066
               Cluster ID: CID-08df6c0d-1db9-40f5-9350-797a3354ff76
           Layout version: -63
       isUpgradeFinalized: true
=====================================================
20/03/21 19:08:08 INFO common.Storage: Storage directory /opt/hadoop/hadoop-data/namenode/data has been successfully formatted.
20/03/21 19:08:09 INFO namenode.TransferFsImage: Opening connection to http://ONE:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:10029943:0:CID-08df6c0d-1db9-40f5-9350-797a3354ff76
20/03/21 19:08:09 INFO namenode.TransferFsImage: Image Transfer timeout configured to 600000 milliseconds
20/03/21 19:08:09 INFO namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
20/03/21 19:08:09 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 322 bytes.
20/03/21 19:08:09 INFO util.ExitUtil: Exiting with status 0
20/03/21 19:08:09 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at TWO/0.0.0.2
************************************************************/
```

### 初始化 ZooKeeper
* Initializing命令
```shell
#zkfc初始化
su - hadoop
cd hadoop-2.7.7/bin/
./hdfs zkfc -formatZK
```

* Initializing日志
关键信息：`ha.ActiveStandbyElector: Successfully created /hadoop-ha/bicluster in ZK.`
```shell
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/bin/
[hadoop@ONE bin]$ ./hdfs zkfc -formatZK
20/03/21 19:12:05 INFO tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at ONE/0.0.0.1:8021
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:host.name=ONE
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_172
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:java.home=/opt/hadoop/jdk1.8.0_172/jre
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/opt/hadoop/hadoop-2.7.7/lib/native
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:os.version=2.6.32-642.4.2.el6.x86_64
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:user.name=hadoop
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:user.home=/opt/hadoop
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Client environment:user.dir=/opt/hadoop/hadoop-2.7.7/bin
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=ONE:2181,TWO:2181,THREE:2181 sessionTimeout=5000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@6ae5aa72
20/03/21 19:12:05 INFO zookeeper.ClientCnxn: Opening socket connection to server TWO/0.0.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
20/03/21 19:12:05 INFO zookeeper.ClientCnxn: Socket connection established to TWO/0.0.0.2:2181, initiating session
20/03/21 19:12:05 INFO zookeeper.ClientCnxn: Session establishment complete on server TWO/0.0.0.2:2181, sessionid = 0x2015eadd8b00000, negotiated timeout = 10000
20/03/21 19:12:05 INFO ha.ActiveStandbyElector: Session connected.
20/03/21 19:12:05 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/bicluster in ZK.
20/03/21 19:12:05 INFO zookeeper.ZooKeeper: Session: 0x2015eadd8b00000 closed
20/03/21 19:12:05 INFO zookeeper.ClientCnxn: EventThread shut down
```

## 启动HDFS
* 启动命令
```
#startup
# 启动了整个集群
su - hadoop
cd hadoop-2.7.7/sbin/
./start-dfs.sh
```

* 启动日志
```shell
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/sbin/
[hadoop@ONE sbin]$ ./start-dfs.sh
Starting namenodes on [ONE TWO]
TWO: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
TWO: starting namenode, logging to /opt/hadoop/hadoop-2.7.7/logs/hadoop-hadoop-namenode-TWO.out
ONE: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
ONE: namenode running as process 25614. Stop it first.
TWO: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
ONE: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
THREE: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
TWO: starting datanode, logging to /opt/hadoop/hadoop-2.7.7/logs/hadoop-hadoop-datanode-TWO.out
ONE: starting datanode, logging to /opt/hadoop/hadoop-2.7.7/logs/hadoop-hadoop-datanode-ONE.out
THREE: starting datanode, logging to /opt/hadoop/hadoop-2.7.7/logs/hadoop-hadoop-datanode-THREE.out
Starting journal nodes [ONE TWO THREE]
ONE: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
THREE: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
TWO: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
ONE: journalnode running as process 26544. Stop it first.
THREE: journalnode running as process 17932. Stop it first.
TWO: journalnode running as process 31081. Stop it first.
Starting ZK Failover Controllers on NN hosts [ONE TWO]
ONE: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
TWO: HADOOP_HOME: /opt/hadoop/hadoop-2.7.7
ONE: starting zkfc, logging to /opt/hadoop/hadoop-2.7.7/logs/hadoop-hadoop-zkfc-ONE.out
TWO: starting zkfc, logging to /opt/hadoop/hadoop-2.7.7/logs/hadoop-hadoop-zkfc-TWO.out
```


### 检查NameNode进程
在namenode nn1和namenode nn2服务器上执行同样的检查。
```SHELL
#ONE
# 进程
[hadoop@ONE sbin]$ ps -ef |grep NameNode | grep -v grep
hadoop   25614     1  0 19:05 pts/0    00:00:36 /opt/hadoop/jdk1.8.0_172/bin/java -Dproc_namenode -Xmx6144m -Xms6144M -d64 -server -XX:+AggressiveOpts -XX:+AlwaysPreTouch -XX:MaxDirectMemorySize=1024M -XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=30 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=8 -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:G1NewSizePercent=10 -XX:+G1SummarizeConcMark -XX:G1LogLevel=finest -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintAdaptiveSizePolicy -Xloggc:/opt/hadoop/hadoop-2.7.7/logs/gc/gc-namenode-20200321-19.05.14.txt -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+HeapDumpOnOutOfMemoryError -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=hadoop-hadoop-namenode-ONE.log -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode

# 端口
[hadoop@ONE sbin]$ ps -ef |grep NameNode | grep -v grep | awk '{ print "ss -lpn | grep "$2}' | sh
LISTEN     0      128            0.0.0.1:8020                     *:*      users:(("java",25614,226))
LISTEN     0      128            0.0.0.1:8021                     *:*      users:(("java",25614,216))
LISTEN     0      128            0.0.0.1:50070                    *:*      users:(("java",25614,193))

# 日志
tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-namenode-$(hostname).log
```

### 检查JournalNode进程
> 在安装了JournalNode的三台服务器上执行检查
```SHELL
#ONE
#进程
[hadoop@ONE sbin]$ ps -ef | grep JournalNode | grep -v grep
hadoop   26544     1  0 18:51 pts/0    00:00:17 /opt/hadoop/jdk1.8.0_172/bin/java -Dproc_journalnode -Xmx1024m -Xms256M -d64 -server -XX:+AggressiveOpts -XX:+AlwaysPreTouch -XX:MaxDirectMemorySize=1024M -XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=30 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=8 -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:G1NewSizePercent=10 -XX:+G1SummarizeConcMark -XX:G1LogLevel=finest -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintAdaptiveSizePolicy -Xloggc:/opt/hadoop/hadoop-2.7.7/logs/gc/gc-journalnode-20200321-18.51.12.txt -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+HeapDumpOnOutOfMemoryError -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=hadoop-hadoop-journalnode-ONE.log -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.qjournal.server.JournalNode

# 端口
[hadoop@ONE sbin]$ ps -ef |grep JournalNode | grep -v grep | awk '{ print "ss -lpn | grep "$2}' | sh
LISTEN     0      128                       *:8480                     *:*      users:(("java",26544,193))
LISTEN     0      128                       *:8485                     *:*      users:(("java",26544,198))

# 日志
tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-journalnode-$(hostname).log
```

### 检查DFSZKFailoverController进程
```shell
#ONE
# 进程
[hadoop@ONE sbin]$ ps -ef | grep DFSZKFailoverController | grep -v grep
hadoop    1773     1  0 22:48 ?        00:00:05 /opt/hadoop/jdk1.8.0_172/bin/java -Dproc_zkfc -Xmx1024m -Xms256M -d64 -server -XX:+AggressiveOpts -XX:+AlwaysPreTouch -XX:MaxDirectMemorySize=1024M -XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=30 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=8 -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:G1NewSizePercent=10 -XX:+G1SummarizeConcMark -XX:G1LogLevel=finest -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintAdaptiveSizePolicy -Xloggc:/opt/hadoop/hadoop-2.7.7/logs/gc/gc-zkfc-20200321-22.48.12.txt -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+HeapDumpOnOutOfMemoryError -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=hadoop-hadoop-zkfc-ONE.log -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.tools.DFSZKFailoverController

# 端口
[hadoop@ONE sbin]$ ps -ef |grep DFSZKFailoverController | grep -v grep | awk '{ print "ss -lpn | grep "$2}' | sh
LISTEN     0      128            0.0.0.1:8019                     *:*      users:(("java",1773,191))

# 日志
tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-zkfc-$(hostname).log

# 日志很关键
# active namenode
[hadoop@TWO ~]$ tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-zkfc-$(hostname).log
2020-03-21 22:48:13,701 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8019
2020-03-21 22:48:13,727 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-03-21 22:48:13,727 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8019: starting
2020-03-21 22:48:13,842 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY
2020-03-21 22:48:13,842 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at TWO/0.0.0.2:8021 entered state: SERVICE_HEALTHY
2020-03-21 22:48:13,882 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2020-03-21 22:48:13,889 INFO org.apache.hadoop.ha.ActiveStandbyElector: No old node to fence
2020-03-21 22:48:13,889 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/bicluster/ActiveBreadCrumb to indicate that the local node is the most recent active...
2020-03-21 22:48:13,907 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at TWO/0.0.0.2:8021 active...
2020-03-21 22:48:14,658 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at TWO/0.0.0.2:8021 to active state

# standby namenode
[hadoop@ONE ~]$ tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-zkfc-$(hostname).log
2020-03-21 22:48:13,893 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at ONE/0.0.0.1:8021 should become standby
2020-03-21 22:48:13,907 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at ONE/0.0.0.1:8021 to standby state
2020-03-23 11:37:14,641 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1015eb5a4ab0000, likely server has closed socket, closing socket connection and attempting reconnect
2020-03-23 11:37:14,744 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2020-03-23 11:37:15,723 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server TWO/0.0.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
2020-03-23 11:37:15,725 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to TWO/0.0.0.2:2181, initiating session
2020-03-23 11:37:15,728 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server TWO/0.0.0.2:2181, sessionid = 0x1015eb5a4ab0000, negotiated timeout = 10000
2020-03-23 11:37:15,729 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2020-03-23 11:37:15,734 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at ONE/0.0.0.1:8021 should become standby
2020-03-23 11:37:15,740 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at ONE/0.0.0.1:8021 to standby state
```

### 检查DataNode进程
在所有安装了DataNode的服务器执行检查。
```shell
#ONE
#进程
[hadoop@ONE sbin]$ ps -ef | grep DataNode | grep -v grep
hadoop     710     1  1 22:48 ?        00:00:13 /opt/hadoop/jdk1.8.0_172/bin/java -Dproc_datanode -Xmx4096m -Xms4096M -d64 -server -XX:+AggressiveOpts -XX:+AlwaysPreTouch -XX:MaxDirectMemorySize=1024M -XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=30 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=8 -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:G1NewSizePercent=10 -XX:+G1SummarizeConcMark -XX:G1LogLevel=finest -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintAdaptiveSizePolicy -Xloggc:/opt/hadoop/hadoop-2.7.7/logs/gc/gc-datanode-20200321-22.48.05.txt -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+HeapDumpOnOutOfMemoryError -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=hadoop-hadoop-datanode-ONE.log -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode

# 端口
[hadoop@ONE sbin]$ ps -ef |grep DataNode | grep -v grep | awk '{ print "ss -lpn | grep "$2}' | sh
LISTEN     0      128                       *:50020                    *:*      users:(("java",710,494))
LISTEN     0      128               127.0.0.1:14359                    *:*      users:(("java",710,198))
LISTEN     0      128                       *:50010                    *:*      users:(("java",710,186))
LISTEN     0      128                       *:50075                    *:*      users:(("java",710,490))

# 日志
tail -F ${HADOOP_HOME}/logs/hadoop-hadoop-datanode-$(hostname).log
```

## 检查HDFS运行情况
### 检查ha状态
```shell
#ONE
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/bin/
[hadoop@ONE bin]$ ./hdfs haadmin -getServiceState nn1
standby
[hadoop@ONE bin]$ ./hdfs haadmin -getServiceState nn2
active
```

### NameNode Web接口
* namenode nn1 web ui : http://ONE:50070/
* namenode nn2 web ui : http://TWO:50070/

### 命令行查看
```SHELL
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/bin/
[hadoop@ONE bin]$ ./hdfs dfsadmin -report
Configured Capacity: 46453004922880 (42.25 TB)
Present Capacity: 45986555252736 (41.82 TB)
DFS Remaining: 45983536640000 (41.82 TB)
DFS Used: 3018612736 (2.81 GB)
DFS Used%: 0.01%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0

-------------------------------------------------
Live datanodes (3):

Name: 0.0.0.2:50010 (TWO)
Hostname: TWO
Decommission Status : Normal
Configured Capacity: 10971532222464 (9.98 TB)
DFS Used: 1613635584 (1.50 GB)
Non DFS Used: 0 (0 B)
DFS Remaining: 10860139732992 (9.88 TB)
DFS Used%: 0.01%
DFS Remaining%: 98.98%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 2
Last contact: Sat Mar 21 23:09:14 CST 2020


Name: 0.0.0.1:50010 (ONE)
Hostname: ONE
Decommission Status : Normal
Configured Capacity: 11010292731904 (10.01 TB)
DFS Used: 565248 (552 KB)
Non DFS Used: 1689808896 (1.57 GB)
DFS Remaining: 10898434486272 (9.91 TB)
DFS Used%: 0.00%
DFS Remaining%: 98.98%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 2
Last contact: Sat Mar 21 23:09:14 CST 2020


Name: 0.0.0.3:50010 (THREE)
Hostname: THREE
Decommission Status : Normal
Configured Capacity: 24471179968512 (22.26 TB)
DFS Used: 1404411904 (1.31 GB)
Non DFS Used: 0 (0 B)
DFS Remaining: 24224962420736 (22.03 TB)
DFS Used%: 0.01%
DFS Remaining%: 98.99%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 2
Last contact: Sat Mar 21 23:09:12 CST 2020
```
## YARN集群启动
启动yarn集群分两步：
* 在NameNode服务器ONE启动 ResourceManager rm1和所有NodeManager。
* 在NameNode服务器TWO启动 ResourceManager rm2

### 启动ResourceManager rm1和NodeManager
```shell
#starup
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/sbin/
[hadoop@ONE sbin]$ ./start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop/hadoop-2.7.7/logs/yarn-hadoop-resourcemanager-ONE.out
ONE: starting nodemanager, logging to /opt/hadoop/hadoop-2.7.7/logs/yarn-hadoop-nodemanager-ONE.out
THREE: starting nodemanager, logging to /opt/hadoop/hadoop-2.7.7/logs/yarn-hadoop-nodemanager-THREE.out
TWO: starting nodemanager, logging to /opt/hadoop/hadoop-2.7.7/logs/yarn-hadoop-nodemanager-TWO.out
```

### 启动ResourceManager rm2
```shell
#starup
[root@TWO ~]# su - hadoop
[hadoop@TWO ~]$ cd hadoop-2.7.7/sbin/
[hadoop@TWO sbin]$ ./yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /opt/hadoop/hadoop-2.7.7/logs/yarn-hadoop-resourcemanager-TWO.out
```

###  检查进程
#### 检查ResourceManager进程
```shell
#ONE
#进程
[hadoop@ONE sbin]$ ps -ef | grep ResourceManager | grep -v grep
hadoop   29672     1  4 23:15 pts/0    00:00:20 /opt/hadoop/jdk1.8.0_172/bin/java -Dproc_resourcemanager -Xmx2048m -Xms1024M -d64 -server -XX:+AggressiveOpts -XX:MaxDirectMemorySize=256M -XX:+UseG1GC -XX:MaxGCPauseMillis=400 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=30 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=4 -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:G1NewSizePercent=20 -XX:+G1SummarizeConcMark -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dyarn.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=yarn-hadoop-resourcemanager-ONE.log -Dyarn.log.file=yarn-hadoop-resourcemanager-ONE.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dyarn.policy.file=hadoop-policy.xml -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dyarn.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=yarn-hadoop-resourcemanager-ONE.log -Dyarn.log.file=yarn-hadoop-resourcemanager-ONE.log -Dyarn.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -classpath /opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/*:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/*:/opt/hadoop/hadoop-2.7.7/etc/hadoop/rm-config/log4j.properties org.apache.hadoop.yarn.server.resourcemanager.ResourceManager

# 端口
[hadoop@ONE sbin]$ ps -ef |grep ResourceManager | grep -v grep | awk '{ print "ss -lpn | grep "$2}' | sh
LISTEN     0      128     ::ffff:0.0.0.1:8030                    :::*      users:(("java",29672,231))
LISTEN     0      128     ::ffff:0.0.0.1:8031                    :::*      users:(("java",29672,217))
LISTEN     0      128     ::ffff:0.0.0.1:8032                    :::*      users:(("java",29672,241))
LISTEN     0      128     ::ffff:0.0.0.1:8033                    :::*      users:(("java",29672,207))
LISTEN     0      128     ::ffff:0.0.0.1:8088                    :::*      users:(("java",29672,200))

# 日志
tail -F ${HADOOP_HOME}/logs/yarn-hadoop-resourcemanager-$(hostname).log
```

####  检查NodeManager进程
```shell
#ONE
#进程
[hadoop@ONE sbin]$ ps -ef | grep NodeManager | grep -v grep
hadoop   29839     1  2 23:15 ?        00:00:17 /opt/hadoop/jdk1.8.0_172/bin/java -Dproc_nodemanager -Xmx2048m -Xms1024M -d64 -server -XX:+AggressiveOpts -XX:MaxDirectMemorySize=256M -XX:+UseG1GC -XX:MaxGCPauseMillis=400 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=30 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=4 -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions -XX:G1NewSizePercent=20 -XX:+G1SummarizeConcMark -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dyarn.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-ONE.log -Dyarn.log.file=yarn-hadoop-nodemanager-ONE.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dyarn.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-ONE.log -Dyarn.log.file=yarn-hadoop-nodemanager-ONE.log -Dyarn.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -classpath /opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/etc/hadoop:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/common/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/hdfs/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/*:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar:/opt/hadoop/hadoop-2.7.7/contrib/capacity-scheduler/*.jar:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/*:/opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/*:/opt/hadoop/hadoop-2.7.7/etc/hadoop/nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager

# 端口
[hadoop@ONE sbin]$ ps -ef |grep NodeManager | grep -v grep | awk '{ print "ss -lpn | grep "$2}' | sh
LISTEN     0      128                      :::21187                   :::*      users:(("java",29839,342))
LISTEN     0      128                      :::8040                    :::*      users:(("java",29839,353))
LISTEN     0      128                      :::8042                    :::*      users:(("java",29839,364))
LISTEN     0      128                      :::13562                   :::*      users:(("java",29839,363))

# 日志
tail -F ${HADOOP_HOME}/logs/yarn-hadoop-nodemanager-$(hostname).log
```

## 检查YARN集群运行情况
### 检查ha状态
```shell
#ONE
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd hadoop-2.7.7/bin/
[hadoop@ONE bin]$ ./yarn rmadmin -getServiceState rm1
active
[hadoop@ONE bin]$ ./yarn rmadmin -getServiceState rm2
standby
```

### ResourceManager rm1 Web接口
* ResourceManager rm1 web ui : http://ONE:8088/
* ResourceManager rm web ui : http://TWO:8088/

## JobHistoryServer启动
### 启动JobHistoryServer
> 因为JobHistoryServer配置在rm1，所以登录ONE服务器启动JobHistoryServer
```shell
#启动
[root@ONE ~]# su - hadoop
[hadoop@ONE ~]$ cd /opt/hadoop/hadoop-2.7.7/sbin/
[hadoop@ONE sbin]$ ./mr-jobhistory-daemon.sh start historyserver
```

### 检查进程
```shell
#ONE
#进程
[hadoop@ONE sbin]$ ps -ef | grep JobHistoryServer | grep -v grep
hadoop    4729     1 60 23:32 pts/0    00:00:15 /opt/hadoop/jdk1.8.0_172/bin/java -Dproc_historyserver -Xmx1000m -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=hadoop.log -Dhadoop.root.logger=INFO,console -Dhadoop.id.str=hadoop -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/hadoop/hadoop-2.7.7 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/hadoop/hadoop-2.7.7/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/hadoop/hadoop-2.7.7/logs -Dhadoop.log.file=mapred-hadoop-historyserver-ONE.log -Dhadoop.root.logger=INFO,RFA -Dmapred.jobsummary.logger=INFO,JSA -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer

# 端口
[hadoop@ONE sbin]$ ps -ef |grep JobHistoryServer | grep -v grep | awk '{ print "ss -lpn | grep "$2}' | sh
LISTEN     0      128            0.0.0.1:10020                    *:*      users:(("java",4729,219))
LISTEN     0      128            0.0.0.1:19888                    *:*      users:(("java",4729,213))
LISTEN     0      128                       *:10033                    *:*      users:(("java",4729,202))

# 日志
tail -F ${HADOOP_HOME}/logs/mapred-hadoop-historyserver-$(hostname).log
```

### Web UI
* http://ONE:19888

### 命令行查看
```shell
[hadoop@ONE bin]$ yarn node -list
Total Nodes:3
         Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers
 THREE:13138                RUNNING    THREE:8042                                  0
    TWO:24576                RUNNING       TWO:8042                                  0
    ONE:21187                RUNNING       ONE:8042                                  0
```

## datanode 下线
在namenode 服务器下查看 hdfs-site.xml 文件
```shell
...
 <property>
    <name>dfs.hosts.exclude</name>
    <value>${hadoop.conf.dir}/dfs-exclude</value>
  </property>
...
```
* 在编辑配置好的文件,输入需要下线的 datanode
```shell
[hadoop@BY-DSI hadoop]$ vi dfs-exclude
FOUR
~
~
```

### 刷新namenode
在active NameNode上刷新执行刷新命令
```shell
[root@ZX-DSI hadoop]# hdfs dfsadmin -refreshNodes
```
通过web界面查看要退出的`datanode`状态，也可通过`hdfs dfsadmin -report` 命令来查看
* Datanode状态由Decommission in progress到Decommissioned过程中，会将要退出的datanode上的数据复制到其他datanode上，防止数据丢失。等datanode状态变为Decommissioned后就可以停止datanode服务。

### 停止datanode
在退出的datanode服务器上停止datanode
```shell
[root@FOUR sbin]# ./hadoop-daemon.sh stop datanode
HADOOP_HOME: /opt/wacos/server/hadoop-2.7.7
stopping datanode
```
停止datanode后，查看datanode状态变为dead

## 全量配置文件
&emsp;配置项太多，不一一赘述，根据自己需求查看文档进行配置
* [core-site.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)
配置Hadoop中的核心参数
* [hdfs-site.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)
配置 `hdfs`集群, `namenode`, `datanode`节点，`JournalNode`，`ZKFC`
* [mapred-site.xml](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)
配置`JobHistoryServer`
* [yarn-site.xml](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)
配置`ResourceManager`,`NodeManager`
以上文件都有对应的 `.default` 文件，当没配置时会默认取对应 `.default` 中的值

## 常用 Hadoop 启停命令
```shell
./hadoop-daemon.sh start/stop 模块名称
./yarn-daemon.sh start/stop 模块名称
```
